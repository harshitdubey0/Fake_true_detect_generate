# -*- coding: utf-8 -*-
"""(app.py)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IKYNBwcrnm06p7jQUxERh7KtnJXW6T2w
"""

import streamlit as st
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel
import torch
import os
import requests
import nltk # Ensure nltk is imported here, even if download is external

# --- Configuration for Data URLs ---
# These are the direct download links for your True.csv and Fake.csv files from Google Drive.
TRUE_NEWS_URL = "https://drive.google.com/uc?export=download&id=1t1M4CXDfifk1EP7K3_e6vh9I-yslfLTn"
FAKE_NEWS_URL = "https://drive.google.com/uc?export=download&id=1tam_5hrqy1CcZ12mMGlBTw7y-dt3NjWj"

# --- NLTK Data Download (HANDLED EXTERNALLY VIA DOCKERFILE) ---
# The nltk.download() calls are now performed during the Docker build process
# based on the instructions in your Dockerfile and nltk.txt file.
# So, no explicit download function is needed or called here in app.py.

# --- Text Preprocessing Function (Cached for efficiency) ---
@st.cache_data
def preprocess_text(text):
    """Cleans and preprocesses text for the detector."""
    # These NLTK components are expected to be available because of the Dockerfile setup.
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))

    text = str(text).lower()
    text = re.sub(r'[^a-z\s]', '', text)
    words = word_tokenize(text)
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
    return " ".join(words)

# --- Data Download Function ---
@st.cache_resource
def download_data_files(true_url, fake_url):
    """Downloads True.csv and Fake.csv from specified URLs."""
    data_dir = "data_cache"
    os.makedirs(data_dir, exist_ok=True)

    true_news_path = os.path.join(data_dir, "True.csv")
    fake_news_path = os.path.join(data_dir, "Fake.csv")

    if not os.path.exists(true_news_path):
        st.spinner(f"Downloading True.csv from {true_url}...")
        try:
            r = requests.get(true_url, stream=True)
            r.raise_for_status()
            with open(true_news_path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
            st.success("True.csv downloaded!")
        except Exception as e:
            st.error(f"Failed to download True.csv: {e}")
            return None, None

    if not os.path.exists(fake_news_path):
        st.spinner(f"Downloading Fake.csv from {fake_url}...")
        try:
            r = requests.get(fake_url, stream=True)
            r.raise_for_status()
            with open(fake_news_path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
            st.success("Fake.csv downloaded!")
        except Exception as e:
            st.error(f"Failed to download Fake.csv: {e}")
            return None, None

    return true_news_path, fake_news_path

# --- Model Training and Loading ---
@st.cache_resource
def load_and_train_detector_model(true_news_path, fake_news_path):
    """Loads data, preprocesses, and trains the Fake News Detector."""
    st.spinner("Loading and training detector model... This may take a moment.")
    try:
        true_news = pd.read_csv(true_news_path)
        fake_news = pd.read_csv(fake_news_path)

        true_news['label'] = 0
        fake_news['label'] = 1
        df = pd.concat([true_news, fake_news], ignore_index=True)
        df = df.sample(frac=1, random_state=42).reset_index(drop=True)
        df['full_text'] = df['title'].fillna('') + " " + df['text'].fillna('')

        df['processed_text'] = df['full_text'].apply(preprocess_text)

        vectorizer = TfidfVectorizer(max_features=5000)
        X = vectorizer.fit_transform(df['processed_text'])
        y = df['label']

        model = LogisticRegression(max_iter=1000)
        model.fit(X, y)

        st.success("Detector model loaded and trained!")
        return model, vectorizer
    except Exception as e:
        st.error(f"Error loading or training detector: {e}")
        return None, None

@st.cache_resource
def load_generator_pipeline():
    """Loads the pre-trained GPT-2 text generation pipeline."""
    st.spinner("Loading GPT-2 generator model... This may take a moment (first time download).")
    try:
        generator_pipeline = pipeline(
            'text-generation',
            model='gpt2',
            tokenizer='gpt2',
            device=0 if torch.cuda.is_available() else -1
        )
        st.success("GPT-2 generator loaded!")
        return generator_pipeline
    except Exception as e:
        st.error(f"Error loading generator model: {e}")
        st.warning("Ensure you have internet access for the initial download of GPT-2.")
        return None

# --- Main Streamlit App Layout and Theming ---
st.set_page_config(
    page_title="Fake News AI",
    page_icon="üïµÔ∏è",
    layout="wide",
    initial_sidebar_state="collapsed"
)

st.title("üïµÔ∏è Fake News Detector & News Generator ‚úçÔ∏è")
st.markdown("""
    <style>
    .stApp {
        background-color: #0e1117;
        color: #e0e0e0;
    }
    .stButton>button {
        background-color: #4CAF50;
        color: white;
        border-radius: 12px;
        border: none;
        padding: 10px 24px;
        text-align: center;
        text-decoration: none;
        display: inline-block;
        font-size: 16px;
        margin: 4px 2px;
        cursor: pointer;
        transition-duration: 0.4s;
        box-shadow: 0 8px 16px 0 rgba(0,0,0,0.2), 0 6px 20px 0 rgba(0,0,0,0.19);
    }
    .stButton>button:hover {
        background-color: #45a049;
        box-shadow: 0 12px 16px 0 rgba(0,0,0,0.24), 0 17px 50px 0 rgba(0,0,0,0.19);
    }
    .stTextArea textarea {
        background-color: #1a1c20;
        color: #e0e0e0;
        border-radius: 8px;
        border: 1px solid #333;
    }
    .stTextInput input {
        background-color: #1a1c20;
        color: #e0e0e0;
        border-radius: 8px;
        border: 1px solid #333;
    }
    .stSlider .stSliderHandle {
        background-color: #4CAF50;
    }
    .stSlider .stSliderTrack {
        background-color: #333;
    }
    .stAlert {
        border-radius: 8px;
    }
    .stAlert.error {
        background-color: #ff4d4f;
        color: white;
    }
    .stAlert.success {
        background-color: #52c41a;
        color: white;
    }
    .stAlert.warning {
        background-color: #faad14;
        color: white;
    }
    a {
        color: #1890ff;
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    </style>
    """, unsafe_allow_html=True)


st.markdown("""
    Welcome to the Fake News AI prototype! This application demonstrates two key aspects of combating misinformation:
    identifying fake news and understanding how synthetic text can be generated.
""")

# --- Data Download and Model Loading on App Startup ---
true_news_file_path, fake_news_file_path = download_data_files(TRUE_NEWS_URL, FAKE_NEWS_URL)

detector_model, tfidf_vectorizer = None, None
if true_news_file_path and fake_news_file_path:
    detector_model, tfidf_vectorizer = load_and_train_detector_model(true_news_file_path, fake_news_file_path)
else:
    st.error("Could not download necessary data files. Detector will not function.")


generator_pipeline = load_generator_pipeline()

# --- Fake News Detector Section ---
st.header("üîç Fake News Detector")
st.markdown("Enter any news text below to check if it's likely real or fake.")

detector_input = st.text_area("News Article Text:", height=200, key="detector_input")

if st.button("Detect News", key="detect_button"):
    if detector_input and detector_model and tfidf_vectorizer:
        with st.spinner("Analyzing news text..."):
            processed_input = preprocess_text(detector_input)
            vectorized_input = tfidf_vectorizer.transform([processed_input])
            prediction = detector_model.predict(vectorized_input)[0]

            if prediction == 1:
                st.error("üö® This news is likely **FAKE!**")
            else:
                st.success("‚úÖ This news is likely **REAL.**")
    elif not detector_input:
        st.warning("Please enter some text to detect.")
    else:
        st.warning("Detector is not ready. Please check for errors above.")

st.markdown("---")

# --- News Generator Section (Renamed) ---
st.header("‚úçÔ∏è News Generator")
st.markdown("Enter a prompt, and the AI will generate text for you. This demonstrates general text generation capabilities.")

generator_prompt = st.text_input("Enter a prompt (e.g., 'Breaking news from the capital:', 'Scientists discover...'):", key="generator_prompt")
max_gen_length = st.slider("Maximum generated text length:", 50, 500, 150, key="max_length_slider")

if st.button("Generate Text", key="generate_button"):
    if generator_prompt and generator_pipeline:
        with st.spinner("Generating text..."):
            generated_text_output = generator_pipeline(
                generator_prompt,
                max_length=max_gen_length,
                num_return_sequences=1,
                do_sample=True,
                temperature=0.7,
                top_k=50,
                top_p=0.95
            )[0]['generated_text']
        st.text_area("Generated Text:", generated_text_output, height=250, key="generated_output")
    elif not generator_prompt:
        st.warning("Please enter a prompt to generate text.")
    else:
        st.warning("Generator is not ready. Please check for errors above.")

st.markdown("---")
st.caption("Project by Harshit Dubey")
st.caption("Note: The generator uses a base GPT-2 model. For specialized 'fake news' generation, further fine-tuning would be required.")

# --- Connect with Me Section ---
st.markdown("<br>", unsafe_allow_html=True)
st.subheader("Connect with Harshit Dubey")

col1, col2, col3 = st.columns([1,1,1])

with col1:
    st.markdown(
        """
        <a href="https://www.instagram.com/harshitdubey00?igsh=OTJuOGNvbWI4c2Fj" target="_blank" style="text-decoration: none;">
            <img src="https://upload.wikimedia.org/wikipedia/commons/a/a5/Instagram_icon.png" alt="Instagram" width="30" height="30" style="vertical-align: middle; margin-right: 5px;">
            <span style="font-size: 18px; color: #E1306C;">Instagram</span>
        </a>
        """,
        unsafe_allow_html=True
    )

with col2:
    st.markdown(
        """
        <a href="https://github.com/harshitdubey0" target="_blank" style="text-decoration: none;">
            <img src="https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg" alt="GitHub" width="30" height="30" style="vertical-align: middle; margin-right: 5px;">
            <span style="font-size: 18px; color: #6e5494;">GitHub</span>
        </a>
        """,
        unsafe_allow_html=True
    )

with col3:
    st.markdown(
        """
        <a href="mailto:harshitdubey11578@gmail.com" style="text-decoration: none;">
            <img src="https://upload.wikimedia.org/wikipedia/commons/4/4e/Gmail_Icon.png" alt="Email" width="30" height="30" style="vertical-align: middle; margin-right: 5px;">
            <span style="font-size: 18px; color: #DB4437;">Email</span>
        </a>
        """,
        unsafe_allow_html=True
    )
